import string  # used for returning lexical info
import random  # used for exploration of the state space
import numpy as np

# q-table implementation
#
# needs graph:
# representation w/ nodes
# make weights, (the variables below) of doorways global
# these weights really represent the value attributed to that transition
# if the algorithm works correctly, the agent would take the best path at each node
# the value of the paths will be learned during training


# GLOBAL SPACE

# HYPERPARAMETERS
START_NODE = 'a'
TERMINAL_NODE = 'g'
REWARD_FOR_COMPLETION = 100
REWARD_FOR_PARTICIPATION = 1
LEARNING_RATE = 1
DISCOUNT_FACTOR = 1



a2b = 0  # a -> b
a2h = 0  # a -> h

b2a = 0  # b -> a
b2c = 0  # b -> c
b2e = 0  # b -> e

c2b = 0  # c -> b

d2h = 0  # d -> h
d2f = 0  # d -> f

e2b = 0  # e -> b
e2f = 0  # e -> f
e2h = 0  # e -> h

f2d = 0  # f -> d
f2e = 0  # f -> e
f2g = 0  # f -> g

g2f = 0  # g -> f

h2a = 0  # h -> a
h2d = 0  # h -> d
h2e = 0  # h -> e

# the total number of passages in the graph
num_Q_vals = 18


# FUNCTION SPACE

# prints a representation of the room layout of our graph
# it's messy here, but it prints nicely in the terminal
# TODO: update to make this scale with larger input numbers
def show_graph_image():

    global a2b
    global a2h

    global b2a
    global b2c
    global b2e

    global c2b

    global d2h
    global d2f

    global e2b
    global e2f
    global e2h

    global f2d
    global f2e
    global f2g

    global g2f

    global h2a
    global h2d
    global h2e

    # print("| [c]             [g]  |".format())
    # print("| {} {}             {} {}  |".format(b2c, c2b, f2g, g2f))
    # print("| [b] {} {} [e] {} {} [f]  |".format(b2e, e2b, e2f, f2e))
    # print("| {} {}     {} {}     {} {}  |".format(a2b, b2a, e2h, h2e, d2f, f2d))
    # print("| [a] {} {} [h] {} {} [d]  |".format(a2h, h2a, h2d, d2h))


    print('a2b', '-' * a2b)
    print('a2h', '-' * a2h)

    print('b2a', '-' * b2a)
    print('b2c', '-' * b2c)
    print('b2e', '-' * b2e)

    print('c2b', '-' * c2b)

    print('d2h', '-' * d2h)
    print('d2f', '-' * d2f)

    print('e2b', '-' * e2b)
    print('e2f', '-' * e2f)
    print('e2h', '-' * e2h)

    print('f2d', '-' * f2d)
    print('f2e', '-' * f2e)
    print('f2g', '-' * f2g)

    print('g2f', '-' * g2f)

    print('h2a', '-' * h2a)
    print('h2d', '-' * h2d)
    print('h2e', '-' * h2e)

# returns a dictionary corresponding to the current map
# the graph that shows a mapping of a 1 story house w/ rooms as nodes, dict keys, and adjacent rooms as definitions
def house_map():
    graph_house = {
        'a' : ['b', 'h'],
        'b' : ['a', 'c', 'e'],
        'c' : ['b'],
        'd' : ['f', 'h'],
        'e' : ['b', 'f', 'h'],
        'f' : ['d', 'e', 'g'],
        'g' : ['f'],
        'h' : ['a', 'd', 'e']
    }
    return graph_house

# graph: the dictionary that represents the graph, and adjacent num_nodes
# prints the contents of the passed graph
def print_graph(graph_dict):
    for node in graph_dict:
        print(node, graph[node])

# generate table
# table structure of 'home'
def make_table(graph_type='home'):
    if graph_type is 'home':
        a = [a2b, a2h]
        b = [b2a, b2c, b2e]
        c = [c2b]
        d = [d2f, d2h]
        e = [e2b, e2f, e2h]
        f = [f2d, f2e, f2g]
        g = [g2f]
        h = [h2a, h2d, h2e]

        home_table = {'a' : a,
                      'b' : b,
                      'c' : c,
                      'd' : d,
                      'e' : e,
                      'f' : f,
                      'g' : g,
                      'h' : h}
        return home_table
    else :
        print("\ninvalid graph type passed through.\n")
        return 0

# graph node is the name (ex: 'a') of the node we're at
# determine all possible actions, select one at random
# randomize in range of possible indeces
# return one of those indeces
def random_select(graph_dict, table, current):
    # determine length of possible actions, subtract 1 to start at 0
    possible_actions = len(graph_dict[current]) - 1

    # determine a random number within the specified range, including possible actions
    random_index = random.randint(0, possible_actions)

    # select the value at the random index
    selection = graph_dict[current][random_index]
    return selection

# intake an state as a string, output the next state as a string based off of which transition has the highest weight
# this may also be interpreted as the best possible Q selection
def greedy_select(graph_dict, table, current):

    temp_weight = 0 # a temp variable used to find the greatest valued weight in the available weights for this node
    temp_node_index = 0 # the index of the node w/ the best weight will be recorded and transformed into string later
    selection = 'all_zero' # the placeholder for the string name of the next node to transition to

    path_weights_available_in_node = table[current] # returns the array of weights for current node, used in a for loop

    # this loop finds the highest valued transition, and returns the name of the adjacent node to transition to
    for path_weight in path_weights_available_in_node: # weight is the index of the adjacent node

        # if weight not zero, and less than the current temp weight we'll make it the next highest, and set it as our
        # current return as the best possible transition. This will be overwritten in the case a higher/better option
        # arises
        if (path_weight != 0) and (temp_weight < path_weight):

            temp_weight = path_weight

            # store some marker for the weight ("this is the current highest")
            selection = index_to_name(graph_dict, current, temp_node_index) # translate index of node to it's string
        temp_node_index += 1 # variable used to report which index we're currently in needs updating

    if selection == 'all_zero': # this should only trip in the case that all weights were zero, in which case we explroe
        selection = random_select(graph_dict, table, current)

    return selection

# given some index (the index of the path you will take), and the current node, we'll return the next state if
# the specified path (index) is taken
# ex:
# in:   graph_dict, current: 'b', index: 2
# out:  'e'
# graph_dict: dictionary describing all adjacent paths from an entry
# current_node: string that is the name of the current dictionary entry we're at
# index_to_take: integer within realm of possible actions at the current node
def index_to_name(graph_dict, current_node, index_to_take):
    selection = graph_dict[current_node][index_to_take]
    return selection

#  state_new, reward_current, terminate_episode = take_action(state_current, action_current)
# given the current state and the destination state, this function returns the new state, and the computed reward
def take_action(state_current, state_next, terminate_episode):


    if state_next == TERMINAL_NODE:
        reward = REWARD_FOR_COMPLETION # large
        terminate_episode = True
    else:
        reward = REWARD_FOR_PARTICIPATION # small
        # terminate_episode is False, should remain as is

    return [state_next, reward, terminate_episode]

# should be something like semantic sugar for returning the global weight that connects state
# to state_next (which is masked as action for the sake of following traditional Q learning algorithms)
# this method checks the state, and the next state (disguised as an action) and returns the weight that connects them
# it is implemented very poorly, but desperate times call for desperate measures
# this is a consequence of not using a good structure for storing wieghts
def Q(state, action):

    # a:
    if (state == 'a') and (action == 'b'):
        return a2b
    elif (state == 'a') and (action == 'h'):
        return a2h

    # b:
    elif (state == 'b') and (action == 'a'):
        return b2a
    elif (state == 'b') and (action == 'c'):
        return b2c
    elif (state == 'b') and (action == 'e'):
        return b2e

    # c:
    elif (state == 'c') and (action == 'b'):
        return c2b

    # d:
    elif (state == 'd') and (action == 'f'):
        return d2f
    elif (state == 'd') and (action == 'h'):
        return d2h

    # e:
    elif (state == 'e') and (action == 'b'):
        return e2b
    elif (state == 'e') and (action == 'f'):
        return e2f
    elif (state == 'e') and (action == 'h'):
        return e2h

    # f:
    elif (state == 'f') and (action == 'd'):
        return f2d
    elif (state == 'f') and (action == 'e'):
        return f2e
    elif (state == 'f') and (action == 'g'):
        return f2g

    # g:
    elif (state == 'g') and (action == 'f'):
        return g2f

    # h:
    elif (state == 'h') and (action == 'a'):
        return h2a
    elif (state == 'h') and (action == 'd'):
        return h2d
    elif (state == 'h') and (action == 'e'):
        return h2e

    else:
        print("\nerror in weight ID\n")

# intake state and next state, and augment transition weight accordingly
def propagate_learning(state, action, new_weight):
    global a2b
    global a2h

    global b2a
    global b2c
    global b2e

    global c2b

    global d2h
    global d2f

    global e2b
    global e2f
    global e2h

    global f2d
    global f2e
    global f2g

    global g2f

    global h2a
    global h2d
    global h2e


    # a:
    if (state == 'a') and (action == 'b'):
        a2b = new_weight
    elif (state == 'a') and (action == 'h'):
        a2h = new_weight

    # b:
    elif (state == 'b') and (action == 'a'):
        b2a = new_weight
    elif (state == 'b') and (action == 'c'):
        b2c = new_weight
    elif (state == 'b') and (action == 'e'):
        b2e = new_weight

    # c:
    elif (state == 'c') and (action == 'b'):
        c2b = new_weight

    # d:
    elif (state == 'd') and (action == 'f'):
        d2f = new_weight
    elif (state == 'd') and (action == 'h'):
        d2h = new_weight

    # e:
    elif (state == 'e') and (action == 'b'):
        e2b = new_weight
    elif (state == 'e') and (action == 'f'):
        e2f = new_weight
    elif (state == 'e') and (action == 'h'):
        e2h = new_weight

    # f:
    elif (state == 'f') and (action == 'd'):
        f2d = new_weight
    elif (state == 'f') and (action == 'e'):
        f2e = new_weight
    elif (state == 'f') and (action == 'g'):
        f2g = new_weight

    # g:
    elif (state == 'g') and (action == 'f'):
        g2f = new_weight

    # h:
    elif (state == 'h') and (action == 'a'):
        return h2a
    elif (state == 'h') and (action == 'd'):
        return h2d
    elif (state == 'h') and (action == 'e'):
        return h2e

    else:
        print("\nerror in weight assignment\n")
    return

def epsilon_select():
    EXPLORE_LIMIT = (3/8)

    epsilon_rand = np.random.rand()

    if epsilon_rand < EXPLORE_LIMIT:
        return random_select(graph, Q_table, state_current)
    else:
        return greedy_select(graph, Q_table, state_current)


# SARSA (Q-learning) overview:

# initialize Q table: Q(s,a) = rand(size_of_table)
# for each episode
    # initialize state: s = start_node # string, corresponds to dict entry
    # choose action from possible actions at state w/ greed: action = greedy_select(state)
    # for each time step in episode:
        # take action selected earlier, observe:
        # choose another action to take, w/ greed: action_prime = greedy_action(state_prime)
        # TODO: update Q-value: previous_Q(s, a) = (1- LR)Q(s,a) + LR[r + discount*Q(s',a')]
        # set previous state and action to new state and action
    #end when terminal s is acheived
# end when updates to Q are nominal


# main:

# generate graph

graph = house_map()

# show graph: image
print("\nGraph as image: ")
show_graph_image()

# show graph: dict
print("\nGraph as dictionary: ")
print_graph(graph)


# initialize Q table randomly (chose to do as zeros):
Q_table = make_table(graph_type='home')  ; print("\nQ_table: \n", Q_table)  # some debugging info

# this value will be updated upon the end of training; will be determined based on how much Q(S,A) updates
# [denotes convergance]
terminate_learning = False

# denotes flag that terminates episode in the while loop. This will be changed at the point the agent reaches the goal
terminate_episode = False

# "while in training mode: "
learning_counter = 0
while terminate_learning is False:
    # learning_counter += 1
    # if learning_counter > 10:
    #     break
    cumilative_episode_reward = 0 # reward that will be incremented during training

    # initialize the starting node; start: c, end: g
    state_current = START_NODE

    # action current holds a node selected from the start state


    # action_current = greedy_select(graph, Q_table, state_current)

    action_current = epsilon_select()

    episode_counter = 0
    while terminate_episode is False:

            # this function returns reward
            state_new, reward_current, terminate_episode = take_action(state_current, action_current, terminate_episode)

            cumilative_episode_reward += reward_current

            # at this time, state_new is the state we are in, and state_current is the previous one
            # action_current holds the state after state_new
            action_new = greedy_select(graph, Q_table, state_new)

            # update Q value:
            # TODO: update Q-value: Q(s, a) = (1- LR)Q(s,a) + LR[r + discount*Q(s',a')]
            # TODO: Q(s, a) is interpreted as weight(state, nest_state). There is a Q table entry for this in our Q_table

            # copies current Qval
            current_weight = Q(state_current, action_current)

            # copies new Qva;

            new_weight = Q(state_new, action_new)

            print(state_new)
            print(action_new)

            # computes learning step in current Qval using current and new Qval
            current_weight = (((1 - LEARNING_RATE)*current_weight) + \
                             (LEARNING_RATE)*(cumilative_episode_reward + (DISCOUNT_FACTOR*new_weight)))

            # reflect learning on current Qval (man i wish i had pointers)
            propagate_learning(state_current, action_current, current_weight)

            state_current = state_new
            action_current = action_new

            print("\nTime: {}\n".format(episode_counter))
            show_graph_image()

            episode_counter += 1

# debug
#